My notes to self on Autoscoring code: 

* June 15, 2015 
It would probably be a good idea to have the autoscoring treat artefact as another state.  When there is an artefact, the EEG and EMG are complete junk (and sometimes ones) so that messes up the autoscoring.  It would probably do a better job if it just classified artefacts as artefacts.  Two ways I could do this: 1) leave the artefacts alone and just run the autoscoring on the other epochs that are not artefacts.  or 2) treat artefact as a fourth category in addition to SWS, wake and REM.  
I'm leaning toward doing option 1.  

* June 25, 2015
12:48 It seems like the classify_usingPCA is now ignoring epochs marked with an X.  I also modified write_scored_file.m to write in an X if the value is a 5 at that 
location.  It still seems funny that this rat file isn't working well.  The PCA plots look good: the cluster separate out nicely for the different states.  Random Forest 
works well.  Why is it classifying everything (non-artefact) as wake?  

1:10 I am now outputing the posterior conditional probabilities.  For a good file (like a BA or BL) the numbers are reasonable: each row corresponds to an epoch, 
and there are 3 columns: probability that that epoch is wake, probability it is SWS, probability it is REMS.  When I output this for the Rat dataset I get 
NaNs for every element in every row. 

2:00 I've added the ability to draw lines on a PCA plot of the training data.  It works for good data like BA or BL, but for the RAT dataset it doesn't draw any lines and complains.  Keep digging into the myclassify.m to see what is going wrong with the file D:\mrempe\autoscoring_for_Will\Dec2014\Rat10_BL_16-00to22-00.txt 

* June 26, 2015
8:56 One idea: compute PCA vectors only on non-artefact data.  This may help, but is seems strange that the PCA plots still look so good. Naive Bayes should still work.  
I found the problem:  Some of the EMG data were missing.  This meant that the PCA vector had NaNs for those entries which messed up everything in classify.m. I made it so it first 
checks to see if any data are missing.  If an epoch has any missing data (regardless of whether it is in the training data or not) that epoch will be scored as an artefact and left 
out of the classifying.  It seems to work well now on the one Rat recording and the excel output looks really good.  All artefact epochs are the same as the human-scored file.  
Keep testing it on the mouse data. 

* July 1, 2015
I have updated PCASCOREBATCHMODE.m and classify_usingPCA.m to include the option of taking several random subsets of the training data in an effort to improve agreement. 
Also, I'm requiring a subsample of training data to include at least 10 episodes of REMS, but if it runs for more than two seconds and still can't get a sample with 
10 episodes of REMS, it will relax that restriction to be only 9, then 8, etc.

keep testing Epoch-Based-Processing\Timed_intervals\export\AutomatedScoring\AutoScore_with_GUI on 2-second data and other files.    

* July 2, 2015
I've been working on compare_scoring_in_two_files.m and it may be almost working.  The idea is to make compute_agreement.m more robust by allowing it to handle
cell arrays of characters (W, S, R) instead of just vectors of 0,1,2.  It may be working, but one issue was how to deal with empty scoring in the human-scored file. 
The agreement parameters weren't quite matching up between the PCASCOREBATCHMODE.m and this new version of compute_agreement, but it may be because of missing scoring in the 
human-scored file.  Keep checking this.  You may also try filling in the missing data with W and seeing if that changes to agreement stats in PCASCOREBATCHMODE to match. 
file is autoscore_and_epcoh_length_study_data\BA1213_Manual.txt

* July 6, 2015
I have fixed compare_scoring_in_two_files.m.  This issue was that when you are computing global agreement, and agreement for each state you need to know which one
is considered the standard.  This has always been the human-scored version when I call this function for the autoscoring stuff.  Now, when using this to compare
two human scorers it isn't clear which one should be the standard.  We need to have a standard because each formula (apart from global agreement) computes the 
number of epochs scored as each state by both methods divided by the number of epochs classified as that type by the human scorer.  

So, the best way to use compare_scoring_in_two_files.m is just to compute global agreement and kappa.  The other agreement stats don't really make sense.  

* July 8, 2015
To Do:
XX1) Figure out why some autoscored files don't seem to follow the REM rescoring rule.  Janne said she had one that scored lots of REM following wake (perhaps it wasn't enough 
wake to trigger the rescoring rule? ) FIXED: REMS was happening in the first 3 epochs.  
XX1a) Figure out why file E2753 week 1 doesn't get read in correctly. It seems to work now.  I don't know why it didn't before.   
2) For Rahmi: Compute kappa on all the .txt files in FS1\EGR3 Project\Clozapine\RahmiScoring and Jonthan Scoring.  Do this when they have scored a few more files.  
3) Autoscore all the files in both of the directories from step 2. 
3a) Compute kappa on Jonathan vs Rahmi, Computer vs Jonathan, Computer vs Rahmi, etc.  
XX4) For Michelle: make a spreadsheet that has filenames, and whether lactate is good in 24 hrs, 48 hours, 60 hours, etc.  (Saved as EGR3 project/Lactate EEG/Output Files\lactate_analysis.xls).   Think about what to do for junk lactate signals, (like negative values)
5) From Janne: try including theta/beta ratio as one of the 7 features as a way to better detect REM. 
XX6) Write agreement struct to an excel spreadsheet with filenames in the first column and kappa and global agreement in the next two columns.  Save this spreadsheet 
in FS1\EGR3\Clozapine\NemriVsWisorTxts\  
XX7) Make one excel spreadsheet when autoscoring several files in one directory.  It should have the filenames, kappa, and global agreement and data source info tab. 
8) Find Jon Brenneke's txt2edf function.  May be .m or .py.  on UltraRoss or TDT3 system. It would be very helpful to get autoscoring back into Neuroscore so they 
can check and modify if needed. Update: txt2edf was only for edf files made on the MC_Data tool.  I found zdb.py, but I couldn't get it to run in Windows. It ran
on my laptop, but said "I wasn't able to assign a score" for every epoch of the file.  Will says that a version he has was working recently.  
9) Migrate stuff from D: drive to FS1


* July 16, 2015
I have written a new file write_scored_file_fast.m to write the autoscored file without having to click on the Excel window that asks you if you want to replace the file....
The good news is that it seems to work and you don't have to do any clicking on windows.  The bad news:  it is much slower than write_scored_file.m which uses XL.m. 
It relies on dlmcell.m which is from MATLAB central and writes a cell array to a tab-delimited file.  It seems to work, but is slow.  Maybe I can speed it up.  
I ended up going back to write_scored_file.m.  Mine seemed to work but really it was saved as an excel file with a txt extension (not actually a tab-delimited file) This 
messed up everything else (SLEEPREPORT, etc.) because it couldn't read the autoscored files as tab delimited.  You can open and re-save those .txt files as tab-delimited, 
but it is a pain.  I switched it back to write_scored_file.m.  It works, but you have to close a window that pops up for each file.  The issue was the the 
way write_scored_file.m works, it just copied the existing tab-delimited file and re-wrote one column.  This meant that the final file was also tab-delimited. 
Apparantly how I was doing it (storing everything in a cell array and writing the entire cell array to a .txt file at once) didn't put the tabs in there. 
It would be better if XL.m could be updated to modify saveAs to save as different file types (tab-delimited .txt for instance) but for now I'm just leaving it. 

Keep working on PROCESSLBATCHMODE.m in Process_L_with_GUI so the user can choose between the 3-state model and the 5-state model. Also, some of the plotting 
was looking kind of funny for lactate simulations.  Lots of read circles when it seems like there should be more black and cyan circles sometimes. Keep checking this.
This was because these files had not been scored yet, so every epoch was assumed to be wake.  We were just trying to see if the lactate signal had good dynamics or 
not. 

* July 28, 2015
I've been working on a way to get scoring from a .txt file back into a .zdb file.  The code is in FS1\Rempe\MATLAB\zdb_MATLAB.  The problem is that
the .zdb file of a partially scored file only has as many elements as there are scored rows.  I need to be able to change the size of the .zdb file
(which is really just a SQLite 3 database) to put in the new epochs that have scores.  I've been trying to write a new .zdb file from scratch, but 
I may not be able to get the timestamp and filename and other data in there.  It would be great if I could just add data to the existing .zdb file
using an INSERT INTO command or something.

Try this next time:  load in the .zdb as I'm doing in zdb_get_score.m.  Then determine the length of the recording from the .txt file. Then 
fill in the rest of the scores in the .zdb file using a for loop and and INSERT INTO command.  I will need to write not just the score, but also 
all the other values as well (internal_property,logging_log_entry,scoring_key,temporary_scoring_comment,temporary_scoring_group,temporary_scoring_group_to_comment,
temporary_scoring_group_to_key,temporary_scoring_key,temporary_scoring_marker,temporary_workspace_workspace,workspace_workspace).
12:39 I now am able to add the scoring data to the .zdb database, in the sense that it has 6000 rows before I put in the scoring data, and 29112 when I'm finished. 
This is the correct number of rows, but some things are still not working: the new .zdb file I create is the same size as the original and when I open it in 
NeuroScore, the scoring is still missing.  I wonder if some other variables need to change for the length of the scoring to change.  I may need to add in 
a bunch of empty placeholders for the other values?  This seems unlikely because opening the file in NeuroScore shows that it knows how long it is. It just
doesn't have the scoring for most of it.  

* Sept. 28, 2015
I just had a conversation with Janne and skyping with Jelena in Norway. Things to try for the autoscoring method (at least in NB, and maybe RF too)
1) try 5-9 Hz for Theta.  Jelena uses 6-9 but sometimes 5-8.  I'M CURRENTLY USING 5-9 HZ.
2) Check agreement for E2742week3 using NB with and without the REMS rescoring rule.  I think the rule may be undoing some epochs that were correctly scored as  REMS. Check specifically a segment around 7:10:30.  It captured the transition from SWS to REMS perfectly, but then it switches to W too soon. 
3) We developed a new ad-hoc rule: if a segment is scored as REMS and if there is no increase in EMG, continue to score those epochs as REMS until the EMG changes.  Define an EMG change in the following way:  find the average EMG value of all REMS epochs for this animal.  A significant change in EMG is at least one standard deviation (or two, etc.)
4) AASM defines entry and exit from a REMS episode, and the ending of a REMS episode includes body movement as shown in EMG. 

* Oct. 1, 2015
I turned off the REM rescoring rule for E2742week3 and REM agreement improved only slightly while global and kappa and wake agreement got a bit worse.  
Let's leave the REM rescoring rule in there for now. 

* Oct. 2, 2015
I implemented a new REM rescoring rule that keeps scoring epochs as REM until the EMG changes significantly.  This is only done for REMS episodes that are followed by W, not SWS.  This helped only modestly.   REM agreement, global and kappa all improved slightly, but not much.  There must be other issues going on with REMS that make it so it's not scoring well.  

* Oct. 7, 2015
I'm trying to scale all the data so each column is between 0 and 1 to see if that helps it use EMG as more of a variable.  
12:00: my first stab at normalizing Feature vector (after smoothing) gave me worse agreement as measured by the "explained" output. Now 6 out of the 7 
dimensions are needed, rather than 3.  However, when I do the classification using these PCA vectors, the agreement is much better, particularly for REM. 
kappa goes up to .8914 and REM agreement goes up to .6962 (up from .2152)  This is without either REM rescoring rule.  

2:40: Running it on the week 3 EGR3 data using both REM rules gave very good agreement.  I checked E2742week3 and there are still some REM epochs that are 
not extended far enough.  For one such case the REM segment is only 3 epochs long and the EMG is remarkably stable during those 3 epochs.  Then it jumps 
a bit in the next epoch.  For this case the std is very small, so even a small jump means it is more than one SD and it does not invoke the new REM rescoring rule.  

3:30: One problem: for File EGR3_E2745_Lactate_05_21_2015_M_KO.txt two of the lines drawn by NB to separate regions overlap, so no epochs get scored as REMS, 
even though some were scored as REMS in the training data. This gives a REM agreement of 0, but kappa is still higher than it was before.  The code runs 
now, but for that file it does not score any epochs as REM, so REM agreement is 0 and kappa and global are only OK.  

3:56: I re-ran the tricky files from EGR3 lactate and the agreement is a little better than it was before, but not great.  Results stored in agree_stats_NB_scaled.xlsx. 

* Oct. 12, 2015
To improve REM scoring:  Janne sent me a paper (Frandsen_et_al_2015JSR) where they automated REM sleep detection.  One of the things they used was an amplitude curve:  The used a moving 51-sample window (how many seconds?, what frequency?) For each progression of the moving window the amplitude was determined as the difference between the highest and lowest value in the entire window.  This seems to give a nice measure of EMG actitivity and then they describe various methods to determine (using this measure) when REMS is happening.  Baseline was measured, etc.  This may be helpful in determining when a REMS episode ends.  It seems to be a good way to capture the little blip that happens in the EMG when REMS goes to WAKE.  Try this sliding window idea on my method, but I will probably need to use the .edf file instead of .txt.  Or maybe I can make NeuroScore output something like this.  
Also try Hans' idea of power spectrum and slope.  


* Oct. 19, 2015
I figured out why autoscoring was not doing a good job of catching blips in the EMG that should signify one epoch of W.  I was smoothing the data (including EMG) before I did anything with it and that got rid of all those one-epoch blips.  I have turned off smoothing now, and the REM agreement goes down significantly, but the REMS episodes that Janne told me to check are now scored correctly.  
Try smoothing all EEG signals, but not EMG signal.  This looks very promising.  For the file E2851 Baseline it captured all of the R-W-S transitions that Janne pointed out and the REM agreement was .8692 with the training data.  Global and kappa were very high when comparing to the file that Janne rescored.    

Smoothing looks really good on Feature(:,1) which is delta power.
Smoothing looks really good on Feature(:,2) which is theta power.
Smoothing looks really good on Feature(:,3) which is low beta power.
Smoothing looks really good on Feature(:,4) which is high beta power.


The question is whether 1-epoch blips in any of these signals should signify a change in state.  For EMG, it is fairly common for the EMG signal to be much larger in one epoch than it was in the epoch before or after and this is easily seen when visually scoring.  So it would be easy to score this as Wake (visually).  So the EMG signal should probably not be smoothed.  However, can one tell visually if the power in one frequency band changes in one epoch?  It may make more sense to smooth those and visually you wouldn't see changes that only last one epoch anyway.  

3:08: Talking with Janne: this is nearly good enough.  One transition that it is still having a bit of a hard time with is REMS to Wake.  Janne would say that several times there should be one epoch of W after a REMS episode, but the algorithm went straight to SWS.  I changed the medianfilter to use 4 epochs in each direction instead of 2, and that made things a bit worse for the REM-W-S transitions.  But, Janne said that this only happened in 3 out of 17 occurences. 

I re-ran the autoscoring algorithm for file E2792 (which Janne said was one that was tricky) and kappa=0.8774, GA=0.9336, REM=0.5974.  I think this is an improvement over what I was doing before.  

I fixed a small bug in the code that computes PCA using the normalized features.  Mean and std would return NaN if there was a NaN in one of the features.  Now they are called with the 'omitnan' option. 

I re-ran the autoscoring on EGR3_E2753_Lactate_05_24_2015_F_WT.txt and the results aren't much better: kappa=0.6971, GA=0.8328, REM=0.6009.  

* Wednesday, Oct. 21, 2015
I met with Janne today to talk about epochs that the autoscoring mis-scored.  For one two-epoch sequence, when we look at it in Neuroscore, theta power goes way up (04:50:40) from the first epoch to the second, but in the .txt file, the change is very modest (3109 to 3205).  Is there something wrong with how Neuroscore is outputing the data, or how it visualizes the data in the power bands? CHECK THIS IN NEUROSCORE!

One big issue seems to be smoothing.  Looking at this chunk (04:50:40) instead of going up slightly (before smoothing) it goes down slightly (after smoothing).
Smoothing with a wider filter (8) seems to make the PCA plot less overlapping.  

Smoothing makes a huge difference: removing smoothing makes REM agreement go to 0.0299 for E2792week2.  Try smoothing after I convert the data to z-scores.
This helped quite a bit to smooth after converting to z-scores rather than before.  REM agreement and kappa went up in this case.    

Could also try keeping more of the PCAs.  Maybe 4 or 5 instead of 3.  After normalizing, PCA doesn't seem to compress the data too much.  

* Thursday, Oct. 22, 2015
I tried using diagquadratic instead of diaglinear for the classify.m  (diag is needed for it to truly be Naive Bayes)  The agreement was slighly better using diagquadratic, but REM was quite a bit better.  Check the .txt file and/or .zdb.  

* Wednesday, Oct. 28, 2015
Janne can get some human polysomnography data, but she wanted me to check the other classifiers that are out there.  I have found a few papers (saved on UltraRoss 
in Matlab\mrempe\papers\autoscoring) and they use similar features.  

* Thursday, Oct. 29, 2015
I ran Naive Bayes on E2792week2 and checked all of the issues that Janne originally pointed out.  Nearly all of them are perfect, but the one W epoch after a long R segment is still not quite right in 2 or 3 instances.  R agreement was .7836 and Kappa=.8865 without doing random subsets of the training data.  When I ran it using random subsets of the training data the agreement improved to R=0.8116 and kappa=0.8996.  I ran it again with a random subset (10 trials, 0.5 of training data) and agreement was even better: R=0.85, kappa=0.9040.  

I met with Janne and almost everything looks great.  Try the same version of the code on E3034week2. (I did this file and the agreement was really good) I also tried autoscoring E2792week2 with EMG measured as relative time above a threshold using the Peak feature.  But agreement was about the same and it messed up the REMS episode at 04:50:40.  

* Friday, Oct. 30, 2015
I ran the autoscoring on the next 3 files Janne sent me.  It did pretty well on them, but REM was sometimes not great.  Then I ran it using 10 trials of 50% of training data and that really improved the agreement.  