My notes to self on Autoscoring code: 

* June 15, 2015 
It would probably be a good idea to have the autoscoring treat artefact as another state.  When there is an artefact, the EEG and EMG are complete junk (and sometimes ones) so that messes up the autoscoring.  It would probably do a better job if it just classified artefacts as artefacts.  Two ways I could do this: 1) leave the artefacts alone and just run the autoscoring on the other epochs that are not artefacts.  or 2) treat artefact as a fourth category in addition to SWS, wake and REM.  
I'm leaning toward doing option 1.  

* June 25, 2015
12:48 It seems like the classify_usingPCA is now ignoring epochs marked with an X.  I also modified write_scored_file.m to write in an X if the value is a 5 at that 
location.  It still seems funny that this rat file isn't working well.  The PCA plots look good: the cluster separate out nicely for the different states.  Random Forest 
works well.  Why is it classifying everything (non-artefact) as wake?  

1:10 I am now outputing the posterior conditional probabilities.  For a good file (like a BA or BL) the numbers are reasonable: each row corresponds to an epoch, 
and there are 3 columns: probability that that epoch is wake, probability it is SWS, probability it is REMS.  When I output this for the Rat dataset I get 
NaNs for every element in every row. 

2:00 I've added the ability to draw lines on a PCA plot of the training data.  It works for good data like BA or BL, but for the RAT dataset it doesn't draw any lines and complains.  Keep digging into the myclassify.m to see what is going wrong with the file D:\mrempe\autoscoring_for_Will\Dec2014\Rat10_BL_16-00to22-00.txt 

* June 26, 2015
8:56 One idea: compute PCA vectors only on non-artefact data.  This may help, but is seems strange that the PCA plots still look so good. Naive Bayes should still work.  
I found the problem:  Some of the EMG data were missing.  This meant that the PCA vector had NaNs for those entries which messed up everything in classify.m. I made it so it first 
checks to see if any data are missing.  If an epoch has any missing data (regardless of whether it is in the training data or not) that epoch will be scored as an artefact and left 
out of the classifying.  It seems to work well now on the one Rat recording and the excel output looks really good.  All artefact epochs are the same as the human-scored file.  
Keep testing it on the mouse data. 

* July 1, 2015
I have updated PCASCOREBATCHMODE.m and classify_usingPCA.m to include the option of taking several random subsets of the training data in an effort to improve agreement. 
Also, I'm requiring a subsample of training data to include at least 10 episodes of REMS, but if it runs for more than two seconds and still can't get a sample with 
10 episodes of REMS, it will relax that restriction to be only 9, then 8, etc.

keep testing Epoch-Based-Processing\Timed_intervals\export\AutomatedScoring\AutoScore_with_GUI on 2-second data and other files.    

* July 2, 2015
I've been working on compare_scoring_in_two_files.m and it may be almost working.  The idea is to make compute_agreement.m more robust by allowing it to handle
cell arrays of characters (W, S, R) instead of just vectors of 0,1,2.  It may be working, but one issue was how to deal with empty scoring in the human-scored file. 
The agreement parameters weren't quite matching up between the PCASCOREBATCHMODE.m and this new version of compute_agreement, but it may be because of missing scoring in the 
human-scored file.  Keep checking this.  You may also try filling in the missing data with W and seeing if that changes to agreement stats in PCASCOREBATCHMODE to match. 
file is autoscore_and_epcoh_length_study_data\BA1213_Manual.txt

* July 6, 2015
I have fixed compare_scoring_in_two_files.m.  This issue was that when you are computing global agreement, and agreement for each state you need to know which one
is considered the standard.  This has always been the human-scored version when I call this function for the autoscoring stuff.  Now, when using this to compare
two human scorers it isn't clear which one should be the standard.  We need to have a standard because each formula (apart from global agreement) computes the 
number of epochs scored as each state by both methods divided by the number of epochs classified as that type by the human scorer.  

So, the best way to use compare_scoring_in_two_files.m is just to compute global agreement and kappa.  The other agreement stats don't really make sense.  

* July 8, 2015
To Do:
1) Figure out why some autoscored files don't seem to follow the REM rescoring rule.  Janne said she had one that scored lots of REM following wake (perhaps it wasn't enough 
wake to trigger the rescoring rule? )
1a) Figure out why file E2753 week 1 doesn't get read in correctly.  
2) For Rahmi: Compute kappa on all the .txt files in FS1\EGR3 Project\Clozapine\RahmiScoring and Jonthan Scoring.  
3) Autoscore all the files in both of the directories from step 2. 
3a) Compute kappa on Jonathan vs Rahmi, Computer vs Jonathan, Computer vs Rahmi, etc.  
4) For Michelle: make a spreadsheet that has filenames, and whether lactate is good in 24 hrs, 48 hours, 60 hours, etc.  (Saved as EGR3 project/Lactate EEG/Output Files\lactate_analysis.xls).  Have Michelle come by and look at the lactate signals.  Think about what to do for junk lactate signals, (like negative values)
5) From Janne: try including theta/beta ratio as one of the 7 features as a way to better detect REM. 
6) Write agreement struct to an excel spreadsheet with filenames in the first column and kappa and global agreement in the next two columns.  Save this spreadsheet 
in FS1\EGR3\Clozapine\NemriVsWisorTxts\  
XX7) Make one excel spreadsheet when autoscoring several files in one directory.  It should have the filenames, kappa, and global agreement and data source info tab. 
8) Find Jon Brenneke's txt2edf function.  May be .m or .py.  on UltraRoss or TDT3 system. It would be very helpful to get autoscoring back into Neuroscore so they 
can check and modify if needed.   

